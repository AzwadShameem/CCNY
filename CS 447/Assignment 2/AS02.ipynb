{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "dataFrame = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# Splitting the data into features and labels\n",
    "data = dataFrame.iloc[:,:].values\n",
    "features = data[:,:4]\n",
    "labels = data[:,4]\n",
    "print(features.shape)\n",
    "IA = dataFrame.iloc[:,:].values\n",
    "Cl = IA[:,4]\n",
    "IC = np.unique(IA[:,4])\n",
    "\n",
    "first, second = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "# Adaline Model\n",
    "class AdalineGD(object):\n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.cost_ = []\n",
    "        self.w_ = np.zeros(1)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.w_ = np.zeros(1 + x.shape[1])\n",
    "        self.cost_ = []\n",
    "        for a in range(self.n_iter):\n",
    "            output = self.net_input(x)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * x.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, x):\n",
    "        return np.dot(x, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, x):\n",
    "        return self.net_input(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.where(self.activation(x) >= 0.0, 1, -1)\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        return 1.0 - abs(y - self.predict(x)).sum() / (2.0 * y.size)\n",
    "\n",
    "cdx = np.array([1,2]) # Focusing on the classifications of \"versicolor\" and \"verginica\", class indexes 1 and 2\n",
    "fdx = np.array([0,1,2,3]) # all features\n",
    "\n",
    "i = 0\n",
    "numCl = 0\n",
    "for i in range(0 , len(Cl)):\n",
    "    if Cl[i] == IC[cdx[0]]:\n",
    "        numCl += 1\n",
    "    if Cl[i] == IC[cdx[1]]:\n",
    "        numCl += 1    \n",
    "    i += 1  \n",
    "    \n",
    "NC = np.zeros((numCl))\n",
    "NF = np.zeros((numCl, 4))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for i in range(0 , len(Cl)):\n",
    "    if Cl[i] == IC[cdx[0]]:        \n",
    "        NC[j] = 1\n",
    "        NF[j,0] = IA[i,fdx[0]]\n",
    "        NF[j,1] = IA[i,fdx[1]]\n",
    "        NF[j,2] = IA[i,fdx[2]]\n",
    "        NF[j,3] = IA[i,fdx[3]]\n",
    "        j += 1\n",
    "    if Cl[i] == IC[cdx[1]]:      \n",
    "        NC[j] = -1\n",
    "        NF[j,0] = IA[i,fdx[0]]\n",
    "        NF[j,1] = IA[i,fdx[1]]\n",
    "        NF[j,2] = IA[i,fdx[2]]\n",
    "        NF[j,3] = IA[i,fdx[3]]\n",
    "        j += 1\n",
    "    i += 1\n",
    "    \n",
    "NF_Std = np.copy(NF)\n",
    "NF_Std[:,0] = (NF[:,0] - NF[:,0].mean()) / NF[:,0].std()\n",
    "NF_Std[:,1] = (NF[:,1] - NF[:,1].mean()) / NF[:,1].std()\n",
    "NF_Std[:,2] = (NF[:,2] - NF[:,2].mean()) / NF[:,2].std()\n",
    "NF_Std[:,3] = (NF[:,3] - NF[:,3].mean()) / NF[:,3].std()\n",
    "ada = AdalineGD(n_iter=50, eta=0.003)\n",
    "ada.fit(NF_Std, NC)\n",
    "first.append(ada.accuracy(NF_Std, NC))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression().fit(features, labels)\n",
    "first.append(lr.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Linear) Model\n",
    "linear_svm = svm.SVC(kernel='linear').fit(features, labels)\n",
    "first.append(linear_svm.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Polynomial) Model\n",
    "poly_svm = svm.SVC(kernel='poly').fit(features, labels)\n",
    "first.append(poly_svm.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Polynomial) Model\n",
    "rbf_svm = svm.SVC(kernel='rbf').fit(features, labels)\n",
    "first.append(rbf_svm.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Tree Model\n",
    "decision_tree = tree.DecisionTreeClassifier().fit(features, labels)\n",
    "first.append(decision_tree.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ada Boost Model with Decision Trees as base classifier\n",
    "# \"If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\"\n",
    "# Above quote shows that base estimator = None means that DecisionTreeClassifier is used which is what we need\n",
    "ada_boost = AdaBoostClassifier(base_estimator=None).fit(features, labels)\n",
    "first.append(ada_boost.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "random_forest = RandomForestClassifier().fit(features, labels)\n",
    "first.append(random_forest.score(features, labels))\n",
    "print(first[len(first)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on out we will test with 100 train samples and 50 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adaline Model with the train_test_split data\n",
    "xa_train, xa_test, ya_train, ya_test = train_test_split(NF_Std, NC,train_size=(2/3),test_size=(1/3))\n",
    "adaline = AdalineGD(n_iter=50, eta=0.003).fit(xa_train, ya_train)\n",
    "second.append(adaline.accuracy(xa_train, ya_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(adaline.accuracy(xa_test, ya_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the models have been tested. Now we will test all the models again but with the data split by sklearn\n",
    "# We will utilize the train_test_split to split the 150 iris samples into 100 training samples and 50 test samples\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, train_size=(100/150),test_size=(50/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(max_iter=120).fit(x_train, y_train)\n",
    "second.append(lr.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(lr.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Linear) Model\n",
    "linear_svm = svm.SVC(kernel='linear').fit(x_train, y_train)\n",
    "second.append(linear_svm.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(linear_svm.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Polynomial) Model\n",
    "poly_svm = svm.SVC(kernel='poly').fit(x_train, y_train)\n",
    "second.append(poly_svm.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(poly_svm.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (Polynomial) Model\n",
    "rbf_svm = svm.SVC(kernel='rbf').fit(x_train, y_train)\n",
    "second.append(rbf_svm.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(rbf_svm.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree Model\n",
    "decision_tree = tree.DecisionTreeClassifier().fit(x_train, y_train)\n",
    "second.append(decision_tree.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(decision_tree.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ada Boost Model with Decision Trees as base classifier\n",
    "# \"If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\"\n",
    "# Above quote shows that base estimator = None means that DecisionTreeClassifier is used which is what we need\n",
    "ada_boost = AdaBoostClassifier(base_estimator=None).fit(x_train, y_train)\n",
    "second.append(ada_boost.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(ada_boost.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "random_forest = RandomForestClassifier().fit(x_train, y_train)\n",
    "second.append(random_forest.score(x_train, y_train))\n",
    "print(second[len(second)-1])\n",
    "second.append(random_forest.score(x_test, y_test))\n",
    "print(second[len(second)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting The Data Into A Table\n",
    "fig, ax = plt.subplots(dpi=200, figsize = (10,2))\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "cases=np.array([\"Adaline\",\"Logistic Regression\",\"Support Vector Machine (linear)\",\n",
    "                \"Support Vector Machine (Polynomial)\",\"Support Vector Machine (RBF)\",\n",
    "                \"Decision Trees\", \"Ada Boost\", \"Random Forest\"])\n",
    "df = pd.DataFrame({\"Regular\" : first,\n",
    "                   \"Train_Test_Split (x_train)\" : second[0::2],\n",
    "                   \"Train_Test_Split (x_test)\" : second[1::2]})\n",
    "ax.table(cellText=df.values, rowLabels=cases, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to clearly describe and analyze the results of the machine learning models we will first make\n",
    "conclusions each model by itself and then make conclusions by consider all the models in unison.\n",
    "\n",
    "Adaline:  \n",
    "The Adaline model had an 97% accuracy by itself, which shows that the model is very capable by itself. In addtion\n",
    "after using train_test_split with the Adaline model we increase the _train accuracy to 98.48% while the _test accuracy\n",
    "was 94%. there is some over fitting in this particular simulation since there is a 4% difference between _train and _test accuracy.\n",
    "\n",
    "\n",
    "Logistic Regression:  \n",
    "The Logistic Regression model had an accuracy of 97.3% by itself, which means that the model\n",
    "is very capable by itself since the score near 100%. After utilizing the train_test_split method to\n",
    "create the _train and _test samples, the _train accuracy of the model increased to 98%, while the _test\n",
    "accuracy was at 98%. This shows that since the _test accuarcy is 98% which is equal to the _train accuracy 98%, we can say that in this simulation it looks like there wasn't any over fitting.\n",
    "\n",
    "Support Vector Machine (Linear):\n",
    "The Support Vector Machine when using kernel='linear' has an accuracy of 99.3% by itself, which means\n",
    "the model is very capable by itself since the score is near 100%. After utilizing the train_test_split method to split the data into _train and _test, the _train data had an accuracy of 99% and the test data had an accuracy of 98%. Clearly, the _train accuracy is higher than the _train accuracy by 1% which means that there is little to no over fitting in this particular simulation.\n",
    "\n",
    "Support Vector Machine (Polynomial):\n",
    "The Support Vector Machine when using kernel='poly' has an accuracy of 97.3% by itself, which means\n",
    "the model is very capable by itself since the score is near 100%. After utilizing the train_test_split\n",
    "method to split the data into _train and _test, the _train data had an accuracy of 99% and the test data\n",
    "had an accuracy of 98%. This model somewhat benefitted from splitting the data because the _train accuracy\n",
    "increased from 97.3% to 99%. Also, the _test accuracy is 98% which is only 1% different than the _train accuracy\n",
    "so we can say that there was very little or no over fitting in this particular simulation.\n",
    "\n",
    "Support Vector Machine (RBF):\n",
    "The Support Vector Machine when using kernel='poly' has an accuracy of 97.3% by itself, which means\n",
    "the model is very capable by itself since the score is near 100%. After utilizing the train_test_split\n",
    "method to split the data into _train and _test, the _train data had an accuracy of 96% and the test data\n",
    "had an accuracy of 98%. The _test accuracy is 98% which is higher than the 96% _train accuracy\n",
    "so we can say that there was no over fitting in this particular simulation.\n",
    "\n",
    "Decision Trees:\n",
    "The Decision Tree model has an accuracy of 100%, which means that this model is very accurate and capable.\n",
    "After utilizing the train_test_split method to split the data into _train and _test, the _train data had an accuracy \n",
    "of 100% and the _test data had an accuracy of 96%. Clearly, the _test is 4% less than the _test data and therefore,\n",
    "we can say there is some over fitting to the data but not too much since the difference was only 4%. \n",
    "\n",
    "Ada Boost: \n",
    "The Ada Boost model using Decision Trees as base classifier gave us an accuracy of 96% by itself, which means that\n",
    "it is capable by itself because the score is near 100%. After utilizing the train_test_split method to split the data into _train and _test, the _train data had an accuracy of 98% and the _test data had an accuracy of 96%. This shows signs of \n",
    "slight over fitting for our particular simulation because there was 2% difference in _train accuracy and _test accuracy.\n",
    "Also the model was capable of benefitting from the splitting because the split data lead to an increase in 2% of accuracy\n",
    "in the _train results.\n",
    "\n",
    "Random Forest:\n",
    "The Random Forest model gave us an accuracy of 100% by itself, which shows it is very capable and accurate. After utilizing \n",
    "the train_test_split method to split the data into _train and _test, the _train data had an accuracy of 100% and the \n",
    "_test accuracy was 96%. Clearly, the 4% difference in _train and _test accuracies show that there is some slight over fitting in this particular simulation.\n",
    "\n",
    "\n",
    "In Summary:\n",
    "The most capable models by itself seem to be Decision Trees, Random Forest and Support Vector Machine(LINEAR) because\n",
    "they have the top 3 highest accuracies by itself. After utilizing train_test_split method the models usually\n",
    "stayed the same or benefitted from splitting the data. Every model was around 98% to 100%, which is 1%-2% of difference after splitting the data except for the Adaline model and the Support Vector Machine(RBF) model. The models with the most overfitting was the Adaline model, Decision Trees model, and Random Forest model because they gave the highest difference, which was 4% between _train and _test scores. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}